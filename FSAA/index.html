<!DOCTYPE html>
<html lang="en">
  <head>

    <!-- Basic Page Needs
         –––––––––––––––––––––––––––––––––––––––––––––––––– -->
    <meta charset="utf-8">
    <title>Semi-Supervised Few-Shot Atomic Action Recognition</title>
    <meta name="description" content="Project Website of FSAA">
    <meta name="author" content="Sizhe Song">

    <!-- Mobile Specific Metas
         –––––––––––––––––––––––––––––––––––––––––––––––––– -->
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- FONT
         –––––––––––––––––––––––––––––––––––––––––––––––––– -->
    <link href="https://fonts.googleapis.com/css?family=Raleway:400,300,600" rel="stylesheet" type="text/css">

    <!-- CSS
         –––––––––––––––––––––––––––––––––––––––––––––––––– -->
    <link rel="stylesheet" href="./css/normalize.css">
    <link rel="stylesheet" href="./css/skeleton.css">
    <link rel="stylesheet" href="./css/footable.standalone.min.css">

    <!-- Google icon -->
    <link rel="stylesheet" href="https://fonts.googleapis.com/icon?family=Material+Icons">
  </head>
  <body>

    <!-- Primary Page Layout
         –––––––––––––––––––––––––––––––––––––––––––––––––– -->
    <div class="container">
      <h4 style="padding-top:20px; text-align:center">Semi-Supervised Few-Shot Atomic Action Recognition</h4>
      <p align="center", style="margin-bottom:12px;">
        <sup>&#10013</sup><span class="simple" >Sizhe Song</a><sup>1</sup>
        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
        <sup>&#10013</sup><span class="simple" >Xiaoyuan Ni</a><sup>1</sup>
        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
        <span class="simple" >Yu-Wing Tai</a><sup>2</sup>
        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
        <span class="simple" >Chi-Keung Tang</a><sup>1</sup>
      </p>

      <p align="center" style="margin-bottom:20px;">
        <sup>1</sup>The Hong Kong University of Science and Technology
        <span style="display:inline-block; width: 32px"></span>
        <sup>2</sup>Kuaishou
        <span style="display:inline-block; width: 32px"></span>
        <sup>&#10013</sup>Equal Contribution<br>
      </p>


      <!-- <figure>
        <img src="images/objects_teaser.png" style="width:100%"></img>
        <br>
      </figure>
      <div class="caption">
        <b>Sample objects from our ScanObjectNN dataset.</b> The dataset
        contains ~15,000 objects that are categorized into 15 categories with
        2902 unique object instances.  The raw objects are represented by a list
        of points with global and local coordinates, normals, colors attributes
        and semantic labels. We also provide part annotations, which to the best
        of our knowledge is the first on real-world data.
      </div>

      <br> -->

      <div id="teaser" class="container" style="width:100%; margin:0; padding:0">
        <h5>Abstract</h5>
        <p align="justify">
          Despite excellent progress has been made, the perfor-mance on action recognition still heavily relies on specificdatasets, which are difficult to extend new action classesdue to labor-intensive labeling. Moreover, the high diver-sity in spatio-temporal appearance requires robust and rep-resentative action feature aggregation and attention. To address the above issues, we focus on atomic actions andpropose a novel model for semi-supervised few-shot atomicaction recognition. Our model features unsupervised andcontrastive video embedding, loose action alignment, multi-head feature comparison, and attention-based aggregation,together of which enables action recognition with only afew training examples through extracting more representa-tive features and allowing flexibility in spatial and temporalalignment and variations in the action. Experiments showthat our model can attain high accuracy on representativeatomic action datasets outperforming their respective state-of-the-art classification accuracy in full supervision setting.
          <br>
          <br>
        </p>
      </div>

      <div class="container" style="width:100%; margin:0; padding:0">
        <h5>Model</h5>
        <figure>
          <img src="images/FSAA_model.png" style="width:100%"></img>
          
        </figure>
        <div class="caption">
          <b>Model Architecture.</b> Our model provides fine-grained spatial and temporal video processing with high length flexibility, which embeds thevideo feature and temporally combines the features with TCN. Further, our models provides attention pooling and compares the multi-headrelation. Finally, the CTC and MSE loss enables our model for time-invariant few shot classification training.
        </div>

        <br>

        <div class="container" style="width:100%; margin:0; padding:0">
          <div class="container" style="float: left; width:55%; margin:20; padding:0">
            <figure>
              <img src="images/attention.png" style="width:100%"></img>
            </figure>
            <div class="caption">
              <b>Attention Pooling</b> is a module to compress and refine support features. Different with traditional self-attention, we introduce query feature into the pooling procedure to generate "customized" support for each query. Also, we can simply swap support and query so that this module can be used to refine query feature, which we call mutual refinement.
            </div>
          </div>

          <div class="container" style="float: right; width:40%; margin:20; padding:0">
            <figure>
              <img src="images/preprocessing.png" style="width:100%"></img>
            </figure>
            <div class="caption">
              <b>Data Normalization.</b> We use a human-tracking tool to obtain human-centric video tubes, which are frame-wise bounding boxes of human bodywith individual’s identity labeled in each box. We then crop the original frames to get nor-malized images which precisely capture the movement ofeach individual in the video.
            </div>
          </div>
        </div>
      </div>

      <br>

      <div class="container" style="width:100%; margin:0; padding:0">
        <p>More details of our model structure can be found in the <a href="https://sausage-song.github.io/home/files/FSAA.pdf">paper</a> or in the <a href="https://github.com/Sausage-SONG/Few-shot-action-recognition">codes</a>.</p>
      </div>

    
    <div class="section">
      <h5>Performance</h5>
        <div class="container" style="width:100%; margin:0; padding:0">
          <figure>
            <img src="images/result.png" style="width:80%"></img>
          </figure>

          <div class="caption">
            <b>Accuracy (%)</b> of our model compared with other state-of-the-art methods on three datasets, which shows that our few-shotmodel has a leading performance compared with the state-of-the-art methods on all the three datasets trained in fullsupervision. Note that our model is few-shot which has ac-cess to only very limited amount of data.
          </div>
        </div>
      
    <br>
    </div>
    <br>
</div>

    <!-- End Document
         –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  </body>
</html>
